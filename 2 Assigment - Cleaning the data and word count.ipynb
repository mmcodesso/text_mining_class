{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Assigment\n",
    "\n",
    "Now that we already have the data from previous assgiment, let import the files and start to work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the librarys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./data/raw/10-K_Data.pik')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>company_name</th>\n",
       "      <th>form_type</th>\n",
       "      <th>cik</th>\n",
       "      <th>date</th>\n",
       "      <th>file</th>\n",
       "      <th>corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>275433</td>\n",
       "      <td>TRAVELZOO INC</td>\n",
       "      <td>10-K</td>\n",
       "      <td>1133311</td>\n",
       "      <td>2017-03-15</td>\n",
       "      <td>edgar/data/1133311/0001133311-17-000010.txt</td>\n",
       "      <td>b'&lt;SEC-DOCUMENT&gt;0001133311-17-000010.txt : 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2116</td>\n",
       "      <td>ACCO BRANDS Corp</td>\n",
       "      <td>10-K</td>\n",
       "      <td>712034</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>edgar/data/712034/0000712034-17-000012.txt</td>\n",
       "      <td>b'&lt;SEC-DOCUMENT&gt;0000712034-17-000012.txt : 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>127781</td>\n",
       "      <td>GrowGeneration Corp.</td>\n",
       "      <td>10-K</td>\n",
       "      <td>1604868</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>edgar/data/1604868/0001213900-17-003102.txt</td>\n",
       "      <td>b'&lt;SEC-DOCUMENT&gt;0001213900-17-003102.txt : 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16371</td>\n",
       "      <td>Advanced Biomedical Technologies Inc.</td>\n",
       "      <td>10-K</td>\n",
       "      <td>1385799</td>\n",
       "      <td>2017-02-14</td>\n",
       "      <td>edgar/data/1385799/0001387131-17-000831.txt</td>\n",
       "      <td>b'&lt;SEC-DOCUMENT&gt;0001387131-17-000831.txt : 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230264</td>\n",
       "      <td>Primerica, Inc.</td>\n",
       "      <td>10-K</td>\n",
       "      <td>1475922</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>edgar/data/1475922/0001564590-17-002594.txt</td>\n",
       "      <td>b'&lt;SEC-DOCUMENT&gt;0001564590-17-002594.txt : 201...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                           company_name form_type      cik  \\\n",
       "0  275433                          TRAVELZOO INC      10-K  1133311   \n",
       "1    2116                       ACCO BRANDS Corp      10-K   712034   \n",
       "2  127781                   GrowGeneration Corp.      10-K  1604868   \n",
       "3   16371  Advanced Biomedical Technologies Inc.      10-K  1385799   \n",
       "4  230264                        Primerica, Inc.      10-K  1475922   \n",
       "\n",
       "         date                                         file  \\\n",
       "0  2017-03-15  edgar/data/1133311/0001133311-17-000010.txt   \n",
       "1  2017-02-27   edgar/data/712034/0000712034-17-000012.txt   \n",
       "2  2017-03-31  edgar/data/1604868/0001213900-17-003102.txt   \n",
       "3  2017-02-14  edgar/data/1385799/0001387131-17-000831.txt   \n",
       "4  2017-02-27  edgar/data/1475922/0001564590-17-002594.txt   \n",
       "\n",
       "                                              corpus  \n",
       "0  b'<SEC-DOCUMENT>0001133311-17-000010.txt : 201...  \n",
       "1  b'<SEC-DOCUMENT>0000712034-17-000012.txt : 201...  \n",
       "2  b'<SEC-DOCUMENT>0001213900-17-003102.txt : 201...  \n",
       "3  b'<SEC-DOCUMENT>0001387131-17-000831.txt : 201...  \n",
       "4  b'<SEC-DOCUMENT>0001564590-17-002594.txt : 201...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the html tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files we have imported have html tags and it is very dificult to read, lets tale a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'15\\nDATE AS OF CHANGE:\\t\\t20170315\\n\\nFILER:\\n\\n\\tCOMPANY DATA:\\t\\n\\t\\tCOMPANY CONFORMED NAME:\\t\\t\\tTRAVELZOO INC\\n\\t\\tCENTRAL INDEX KEY:\\t\\t\\t0001133311\\n\\t\\tSTANDARD INDUSTRIAL CLASSIFICATION:\\tSERVICES-COMPUTER INTEGRATED SYSTEMS DESIGN [7373]\\n\\t\\tIRS NUMBER:\\t\\t\\t\\t364415727\\n\\t\\tSTATE OF INCORPORATION:\\t\\t\\tDE\\n\\t\\tFISCAL YEAR END:\\t\\t\\t1231\\n\\n\\tFILING VALUES:\\n\\t\\tFORM TYPE:\\t\\t10-K\\n\\t\\tSEC ACT:\\t\\t1934 Act\\n\\t\\tSEC FILE NUMBER:\\t000-50171\\n\\t\\tFILM N'\n"
     ]
    }
   ],
   "source": [
    "sample = data['corpus'][0]\n",
    "print(sample[300:700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import html2text\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_process( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string\n",
    "    \n",
    "    # 1. Remove HTML\n",
    "    #review_text = html2text.html2text(str(raw_review))\n",
    "    review_text = BeautifulSoup(raw_review,\"html5lib\").get_text() \n",
    "    \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub('\\s+', ' ',review_text)    \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "\n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'orporation de fiscal year end filing values form type k sec act act sec file number film number business address street madison avenue street th floor city new york state ny zip business phone mail address street madison avenue street th floor city new york state ny zip k tzoo x k htm k document united statessecurities exchange commissionwashington c form k mark one xannual report pursuant section'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to make sure its working\n",
    "sample_clean = data['corpus'].head(1).apply(text_process)\n",
    "list(sample_clean)[0][300:700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% ... Documents processed: 100 time running: 105 minutes."
     ]
    }
   ],
   "source": [
    "import sys #to print \n",
    "import time #to get the time\n",
    "\n",
    "# Get the number of documents based on the dataframe column size\n",
    "num_documents = data['corpus'].count()\n",
    "corpus = data['corpus']\n",
    "\n",
    "# Initialize an empty list to hold the clean Documents\n",
    "clean_corpus = []\n",
    "\n",
    "#Start the clock\n",
    "start = time.time()\n",
    "count = 0\n",
    "\n",
    "# Loop over each document; create an index i that goes from 0 to the \n",
    "#length of documents \n",
    "for i in range( 0, num_documents ):\n",
    "    count += 1\n",
    "    # Call our function for each one, and add the result to the \n",
    "    #list of clena documents\n",
    "    clean_corpus.append( text_process( corpus[i] ) )\n",
    "    \n",
    "    \n",
    "    # Printing out the the progress\n",
    "    end = time.time()\n",
    "    sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * count/float(num_documents)) \\\n",
    "                     + \"% ... Documents processed: \" + str(count) \\\n",
    "                     + \" time running: \" + str(int((end-start)/60)) + \" minutes.\") \n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_corpus_df = pd.DataFrame(clean_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['corpus'] = clean_corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets save the file already clened, so next time we can start from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_pickle('./data/10k-clean.pik')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the new data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./data/10k-clean.pik')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating word count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets download a list of Negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_neg = 'https://raw.githubusercontent.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107/08a269765a6b185d5f3dd522c876043ba9628715/data/opinion-lexicon-English/negative-words.txt'\n",
    "neg_words = pd.read_fwf(url_neg,encoding='latin-1',skiprows=34,header=None, \n",
    "                       names=['negatives'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negatives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2-faced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2-faces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abolish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abominable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    negatives\n",
       "0     2-faced\n",
       "1     2-faces\n",
       "2    abnormal\n",
       "3     abolish\n",
       "4  abominable"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Create columns for stemmed and lemmanized negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_stemmed = []\n",
    "neg_lemmatized = []\n",
    "\n",
    "for word in neg_words['negatives']:\n",
    "    neg_stemmed.append(stemmer.stem(word))\n",
    "    neg_lemmatized.append(WordNetLemmatizer.lemmatize(word,word))\n",
    "\n",
    "neg_words['stemm'] = neg_stemmed\n",
    "neg_words['lemmatizes'] = neg_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negatives</th>\n",
       "      <th>stemm</th>\n",
       "      <th>lemmatizes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2-faced</td>\n",
       "      <td>2-face</td>\n",
       "      <td>2-faced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2-faces</td>\n",
       "      <td>2-face</td>\n",
       "      <td>2-faces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abnormal</td>\n",
       "      <td>abnorm</td>\n",
       "      <td>abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abolish</td>\n",
       "      <td>abolish</td>\n",
       "      <td>abolish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abominable</td>\n",
       "      <td>abomin</td>\n",
       "      <td>abominable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    negatives    stemm  lemmatizes\n",
       "0     2-faced   2-face     2-faced\n",
       "1     2-faces   2-face     2-faces\n",
       "2    abnormal   abnorm    abnormal\n",
       "3     abolish  abolish     abolish\n",
       "4  abominable   abomin  abominable"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% ... Documents processed: 100 time running: 24 minutes."
     ]
    }
   ],
   "source": [
    "import sys #to print status\n",
    "import time #to get the time\n",
    "#Start the clock\n",
    "start = time.time()\n",
    "count = 0\n",
    "num_documents = data['corpus'].count()\n",
    "\n",
    "#Creating a new collum for the corpus stemmed and lemmatized\n",
    "corpus_stemed = []\n",
    "corpus_lemmatized = []\n",
    "\n",
    "for corpus in data['corpus']:\n",
    "    corpus_stemed.append(\" \".join([stemmer.stem(word) \\\n",
    "                                   for word in corpus.split()]))\n",
    "    corpus_lemmatized.append(\" \".join([WordNetLemmatizer.lemmatize(word,word) \\\n",
    "                                   for word in corpus.split()]))\n",
    "    # Printing out the the progress\n",
    "    count += 1\n",
    "    end = time.time()\n",
    "    sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * count/float(num_documents)) \\\n",
    "                     + \"% ... Documents processed: \" + str(count) \\\n",
    "                     + \" time running: \" + str(int((end-start)/60)) + \" minutes.\") \n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    \n",
    "data['stemmed'] = corpus_stemed\n",
    "data['lemmatized'] = corpus_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% ... Documents processed: 100 time running: 13 seconds."
     ]
    }
   ],
   "source": [
    "#Creating a new collum for the corpus word count and unique words\n",
    "\n",
    "import sys #to print status\n",
    "import time #to get the time\n",
    "#Start the clock\n",
    "start = time.time()\n",
    "count = 0\n",
    "num_documents = data['corpus'].count()\n",
    "\n",
    "word_count = []\n",
    "unique_word = []\n",
    "\n",
    "for corpus in data['corpus']:\n",
    "    word_count.append(len(corpus.split()))\n",
    "    unique_word.append(len(set(corpus.split())))\n",
    "    # Printing out the the progress\n",
    "    count += 1\n",
    "    end = time.time()\n",
    "    sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * count/float(num_documents)) \\\n",
    "                     + \"% ... Documents processed: \" + str(count) \\\n",
    "                     + \" time running: \" + str(int((end-start))) + \" seconds.\") \n",
    "    sys.stdout.flush()\n",
    "    \n",
    "data['word count'] = word_count\n",
    "data['unique words'] = unique_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% ... Documents processed: 100 time running: 9 seconds."
     ]
    }
   ],
   "source": [
    "#Creating a new collum for the corpus word count for unique Stemmed\n",
    "\n",
    "start = time.time()\n",
    "count = 0\n",
    "num_documents = data['stemmed'].count()\n",
    "\n",
    "unique_stemed = []\n",
    "\n",
    "for corpus in data['stemmed']:\n",
    "    unique_stemed.append(len(set(corpus.split())))\n",
    "    \n",
    "    # Printing out the the progress\n",
    "    count += 1\n",
    "    end = time.time()\n",
    "    sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * count/float(num_documents)) \\\n",
    "                     + \"% ... Documents processed: \" + str(count) \\\n",
    "                     + \" time running: \" + str(int((end-start))) + \" seconds.\") \n",
    "    sys.stdout.flush()\n",
    "    \n",
    "data['unique stemmed'] = unique_stemed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% ... Documents processed: 100 time running: 9 seconds."
     ]
    }
   ],
   "source": [
    "#Creating a new collum for the corpus word count for unique lemmatizes\n",
    "\n",
    "start = time.time()\n",
    "count = 0\n",
    "num_documents = data['lemmatized'].count()\n",
    "\n",
    "unique_lemmatized = []\n",
    "\n",
    "for corpus in data['lemmatized']:\n",
    "    unique_lemmatized.append(len(set(corpus.split())))\n",
    "    \n",
    "    # Printing out the the progress\n",
    "    count += 1\n",
    "    end = time.time()\n",
    "    sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * count/float(num_documents)) \\\n",
    "                     + \"% ... Documents processed: \" + str(count) \\\n",
    "                     + \" time running: \" + str(int((end-start))) + \" seconds.\") \n",
    "    sys.stdout.flush()\n",
    "    \n",
    "data['unique lemmatizes'] = unique_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Lets Count Negatives Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_word = set(list(neg_words['negatives']))\n",
    "neg_stemmed = set(list(neg_words['stemm']))\n",
    "neg_lemmatizes = set(list(neg_words['lemmatizes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_neg = len([word for word in data['corpus'][0].split() if word in neg_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1733"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% ... Documents processed: 100 time running: 11 seconds."
     ]
    }
   ],
   "source": [
    "#Creating a new collum for Count of negative words\n",
    "\n",
    "import sys #to print status\n",
    "import time #to get the time\n",
    "\n",
    "#Start the clock\n",
    "start = time.time()\n",
    "count = 0\n",
    "num_documents = data['corpus'].count()\n",
    "\n",
    "neg_count = []\n",
    "\n",
    "\n",
    "for corpus in data['corpus']:\n",
    "    neg_count.append(len([word for word in corpus.split() if word in neg_word]))\n",
    "\n",
    "    \n",
    "    # Printing out the the progress\n",
    "    count += 1\n",
    "    end = time.time()\n",
    "    sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * count/float(num_documents)) \\\n",
    "                     + \"% ... Documents processed: \" + str(count) \\\n",
    "                     + \" time running: \" + str(int((end-start))) + \" seconds.\") \n",
    "    sys.stdout.flush()\n",
    "    \n",
    "data['Negative words'] = neg_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% ... Documents processed: 100 time running: 11 seconds."
     ]
    }
   ],
   "source": [
    "#Creating a new collum for Count of negative words\n",
    "\n",
    "import sys #to print status\n",
    "import time #to get the time\n",
    "\n",
    "#Start the clock\n",
    "start = time.time()\n",
    "count = 0\n",
    "num_documents = data['stemmed'].count()\n",
    "\n",
    "neg_count = []\n",
    "\n",
    "\n",
    "for corpus in data['stemmed']:\n",
    "    neg_count.append(len([word for word in corpus.split() if word in neg_stemmed]))\n",
    "\n",
    "    \n",
    "    # Printing out the the progress\n",
    "    count += 1\n",
    "    end = time.time()\n",
    "    sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * count/float(num_documents)) \\\n",
    "                     + \"% ... Documents processed: \" + str(count) \\\n",
    "                     + \" time running: \" + str(int((end-start))) + \" seconds.\") \n",
    "    sys.stdout.flush()\n",
    "    \n",
    "data['Negative Stemmed'] = neg_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% ... Documents processed: 100 time running: 10 seconds."
     ]
    }
   ],
   "source": [
    "#Creating a new collum for Count of negative words\n",
    "\n",
    "import sys #to print status\n",
    "import time #to get the time\n",
    "\n",
    "#Start the clock\n",
    "start = time.time()\n",
    "count = 0\n",
    "num_documents = data['lemmatized'].count()\n",
    "\n",
    "neg_count = []\n",
    "\n",
    "\n",
    "for corpus in data['lemmatized']:\n",
    "    neg_count.append(len([word for word in corpus.split() if word in neg_lemmatizes]))\n",
    "\n",
    "    \n",
    "    # Printing out the the progress\n",
    "    count += 1\n",
    "    end = time.time()\n",
    "    sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * count/float(num_documents)) \\\n",
    "                     + \"% ... Documents processed: \" + str(count) \\\n",
    "                     + \" time running: \" + str(int((end-start))) + \" seconds.\") \n",
    "    sys.stdout.flush()\n",
    "    \n",
    "data['Negative lemmatized'] = neg_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>company_name</th>\n",
       "      <th>form_type</th>\n",
       "      <th>cik</th>\n",
       "      <th>date</th>\n",
       "      <th>file</th>\n",
       "      <th>corpus</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>word count</th>\n",
       "      <th>unique words</th>\n",
       "      <th>unique stemmed</th>\n",
       "      <th>unique lemmatizes</th>\n",
       "      <th>Negative words</th>\n",
       "      <th>Negative Stemmed</th>\n",
       "      <th>Negative lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>275433</td>\n",
       "      <td>TRAVELZOO INC</td>\n",
       "      <td>10-K</td>\n",
       "      <td>1133311</td>\n",
       "      <td>2017-03-15</td>\n",
       "      <td>edgar/data/1133311/0001133311-17-000010.txt</td>\n",
       "      <td>txt hdr sgml accession number conformed submis...</td>\n",
       "      <td>txt hdr sgml access number conform submiss typ...</td>\n",
       "      <td>txt hdr sgml accession number conformed submis...</td>\n",
       "      <td>327222</td>\n",
       "      <td>12039</td>\n",
       "      <td>10608</td>\n",
       "      <td>11529</td>\n",
       "      <td>1733</td>\n",
       "      <td>4309</td>\n",
       "      <td>3179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2116</td>\n",
       "      <td>ACCO BRANDS Corp</td>\n",
       "      <td>10-K</td>\n",
       "      <td>712034</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>edgar/data/712034/0000712034-17-000012.txt</td>\n",
       "      <td>txt hdr sgml accession number conformed submis...</td>\n",
       "      <td>txt hdr sgml access number conform submiss typ...</td>\n",
       "      <td>txt hdr sgml accession number conformed submis...</td>\n",
       "      <td>841296</td>\n",
       "      <td>28456</td>\n",
       "      <td>25464</td>\n",
       "      <td>27566</td>\n",
       "      <td>5243</td>\n",
       "      <td>12004</td>\n",
       "      <td>10317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>127781</td>\n",
       "      <td>GrowGeneration Corp.</td>\n",
       "      <td>10-K</td>\n",
       "      <td>1604868</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>edgar/data/1604868/0001213900-17-003102.txt</td>\n",
       "      <td>txt hdr sgml accession number conformed submis...</td>\n",
       "      <td>txt hdr sgml access number conform submiss typ...</td>\n",
       "      <td>txt hdr sgml accession number conformed submis...</td>\n",
       "      <td>175052</td>\n",
       "      <td>11434</td>\n",
       "      <td>10226</td>\n",
       "      <td>11011</td>\n",
       "      <td>1178</td>\n",
       "      <td>3157</td>\n",
       "      <td>2550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16371</td>\n",
       "      <td>Advanced Biomedical Technologies Inc.</td>\n",
       "      <td>10-K</td>\n",
       "      <td>1385799</td>\n",
       "      <td>2017-02-14</td>\n",
       "      <td>edgar/data/1385799/0001387131-17-000831.txt</td>\n",
       "      <td>txt hdr sgml accession number conformed submis...</td>\n",
       "      <td>txt hdr sgml access number conform submiss typ...</td>\n",
       "      <td>txt hdr sgml accession number conformed submis...</td>\n",
       "      <td>82050</td>\n",
       "      <td>8008</td>\n",
       "      <td>6845</td>\n",
       "      <td>7601</td>\n",
       "      <td>889</td>\n",
       "      <td>2321</td>\n",
       "      <td>1635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230264</td>\n",
       "      <td>Primerica, Inc.</td>\n",
       "      <td>10-K</td>\n",
       "      <td>1475922</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>edgar/data/1475922/0001564590-17-002594.txt</td>\n",
       "      <td>txt hdr sgml accession number conformed submis...</td>\n",
       "      <td>txt hdr sgml access number conform submiss typ...</td>\n",
       "      <td>txt hdr sgml accession number conformed submis...</td>\n",
       "      <td>132391</td>\n",
       "      <td>10091</td>\n",
       "      <td>7831</td>\n",
       "      <td>9352</td>\n",
       "      <td>3017</td>\n",
       "      <td>6600</td>\n",
       "      <td>4301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                           company_name form_type      cik  \\\n",
       "0  275433                          TRAVELZOO INC      10-K  1133311   \n",
       "1    2116                       ACCO BRANDS Corp      10-K   712034   \n",
       "2  127781                   GrowGeneration Corp.      10-K  1604868   \n",
       "3   16371  Advanced Biomedical Technologies Inc.      10-K  1385799   \n",
       "4  230264                        Primerica, Inc.      10-K  1475922   \n",
       "\n",
       "         date                                         file  \\\n",
       "0  2017-03-15  edgar/data/1133311/0001133311-17-000010.txt   \n",
       "1  2017-02-27   edgar/data/712034/0000712034-17-000012.txt   \n",
       "2  2017-03-31  edgar/data/1604868/0001213900-17-003102.txt   \n",
       "3  2017-02-14  edgar/data/1385799/0001387131-17-000831.txt   \n",
       "4  2017-02-27  edgar/data/1475922/0001564590-17-002594.txt   \n",
       "\n",
       "                                              corpus  \\\n",
       "0  txt hdr sgml accession number conformed submis...   \n",
       "1  txt hdr sgml accession number conformed submis...   \n",
       "2  txt hdr sgml accession number conformed submis...   \n",
       "3  txt hdr sgml accession number conformed submis...   \n",
       "4  txt hdr sgml accession number conformed submis...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  txt hdr sgml access number conform submiss typ...   \n",
       "1  txt hdr sgml access number conform submiss typ...   \n",
       "2  txt hdr sgml access number conform submiss typ...   \n",
       "3  txt hdr sgml access number conform submiss typ...   \n",
       "4  txt hdr sgml access number conform submiss typ...   \n",
       "\n",
       "                                          lemmatized  word count  \\\n",
       "0  txt hdr sgml accession number conformed submis...      327222   \n",
       "1  txt hdr sgml accession number conformed submis...      841296   \n",
       "2  txt hdr sgml accession number conformed submis...      175052   \n",
       "3  txt hdr sgml accession number conformed submis...       82050   \n",
       "4  txt hdr sgml accession number conformed submis...      132391   \n",
       "\n",
       "   unique words  unique stemmed  unique lemmatizes  Negative words  \\\n",
       "0         12039           10608              11529            1733   \n",
       "1         28456           25464              27566            5243   \n",
       "2         11434           10226              11011            1178   \n",
       "3          8008            6845               7601             889   \n",
       "4         10091            7831               9352            3017   \n",
       "\n",
       "   Negative Stemmed  Negative lemmatized  \n",
       "0              4309                 3179  \n",
       "1             12004                10317  \n",
       "2              3157                 2550  \n",
       "3              2321                 1635  \n",
       "4              6600                 4301  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets save our database for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('./data/10k-v2.pik')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will export some columns to csv file, first we will select the columns, than export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name</th>\n",
       "      <th>word count</th>\n",
       "      <th>unique words</th>\n",
       "      <th>unique stemmed</th>\n",
       "      <th>unique lemmatizes</th>\n",
       "      <th>Negative words</th>\n",
       "      <th>Negative Stemmed</th>\n",
       "      <th>Negative lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAVELZOO INC</td>\n",
       "      <td>327222</td>\n",
       "      <td>12039</td>\n",
       "      <td>10608</td>\n",
       "      <td>11529</td>\n",
       "      <td>1733</td>\n",
       "      <td>4309</td>\n",
       "      <td>3179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACCO BRANDS Corp</td>\n",
       "      <td>841296</td>\n",
       "      <td>28456</td>\n",
       "      <td>25464</td>\n",
       "      <td>27566</td>\n",
       "      <td>5243</td>\n",
       "      <td>12004</td>\n",
       "      <td>10317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GrowGeneration Corp.</td>\n",
       "      <td>175052</td>\n",
       "      <td>11434</td>\n",
       "      <td>10226</td>\n",
       "      <td>11011</td>\n",
       "      <td>1178</td>\n",
       "      <td>3157</td>\n",
       "      <td>2550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Advanced Biomedical Technologies Inc.</td>\n",
       "      <td>82050</td>\n",
       "      <td>8008</td>\n",
       "      <td>6845</td>\n",
       "      <td>7601</td>\n",
       "      <td>889</td>\n",
       "      <td>2321</td>\n",
       "      <td>1635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Primerica, Inc.</td>\n",
       "      <td>132391</td>\n",
       "      <td>10091</td>\n",
       "      <td>7831</td>\n",
       "      <td>9352</td>\n",
       "      <td>3017</td>\n",
       "      <td>6600</td>\n",
       "      <td>4301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            company_name  word count  unique words  \\\n",
       "0                          TRAVELZOO INC      327222         12039   \n",
       "1                       ACCO BRANDS Corp      841296         28456   \n",
       "2                   GrowGeneration Corp.      175052         11434   \n",
       "3  Advanced Biomedical Technologies Inc.       82050          8008   \n",
       "4                        Primerica, Inc.      132391         10091   \n",
       "\n",
       "   unique stemmed  unique lemmatizes  Negative words  Negative Stemmed  \\\n",
       "0           10608              11529            1733              4309   \n",
       "1           25464              27566            5243             12004   \n",
       "2           10226              11011            1178              3157   \n",
       "3            6845               7601             889              2321   \n",
       "4            7831               9352            3017              6600   \n",
       "\n",
       "   Negative lemmatized  \n",
       "0                 3179  \n",
       "1                10317  \n",
       "2                 2550  \n",
       "3                 1635  \n",
       "4                 4301  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['company_name','word count','unique words','unique stemmed', \\\n",
    "      'unique lemmatizes', 'Negative words', 'Negative Stemmed', \\\n",
    "      'Negative lemmatized']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#exporting\n",
    "data[['company_name','word count','unique words','unique stemmed', \\\n",
    "      'unique lemmatizes', 'Negative words', 'Negative Stemmed', \\\n",
    "      'Negative lemmatized']].to_csv('./data/word count.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
