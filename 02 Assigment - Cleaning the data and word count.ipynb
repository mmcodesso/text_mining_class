{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment 02\n",
    "\n",
    "Now that we already have the data from previous assgiment, let import the files and start to work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the librarys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./data/raw/10-K_Data.pik')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>company_name</th>\n",
       "      <th>form_type</th>\n",
       "      <th>cik</th>\n",
       "      <th>date</th>\n",
       "      <th>file</th>\n",
       "      <th>corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>275433</td>\n",
       "      <td>TRAVELZOO INC</td>\n",
       "      <td>10-K</td>\n",
       "      <td>1133311</td>\n",
       "      <td>2017-03-15</td>\n",
       "      <td>edgar/data/1133311/0001133311-17-000010.txt</td>\n",
       "      <td>b'&lt;SEC-DOCUMENT&gt;0001133311-17-000010.txt : 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2116</td>\n",
       "      <td>ACCO BRANDS Corp</td>\n",
       "      <td>10-K</td>\n",
       "      <td>712034</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>edgar/data/712034/0000712034-17-000012.txt</td>\n",
       "      <td>b'&lt;SEC-DOCUMENT&gt;0000712034-17-000012.txt : 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>127781</td>\n",
       "      <td>GrowGeneration Corp.</td>\n",
       "      <td>10-K</td>\n",
       "      <td>1604868</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>edgar/data/1604868/0001213900-17-003102.txt</td>\n",
       "      <td>b'&lt;SEC-DOCUMENT&gt;0001213900-17-003102.txt : 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16371</td>\n",
       "      <td>Advanced Biomedical Technologies Inc.</td>\n",
       "      <td>10-K</td>\n",
       "      <td>1385799</td>\n",
       "      <td>2017-02-14</td>\n",
       "      <td>edgar/data/1385799/0001387131-17-000831.txt</td>\n",
       "      <td>b'&lt;SEC-DOCUMENT&gt;0001387131-17-000831.txt : 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230264</td>\n",
       "      <td>Primerica, Inc.</td>\n",
       "      <td>10-K</td>\n",
       "      <td>1475922</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>edgar/data/1475922/0001564590-17-002594.txt</td>\n",
       "      <td>b'&lt;SEC-DOCUMENT&gt;0001564590-17-002594.txt : 201...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                           company_name form_type      cik  \\\n",
       "0  275433                          TRAVELZOO INC      10-K  1133311   \n",
       "1    2116                       ACCO BRANDS Corp      10-K   712034   \n",
       "2  127781                   GrowGeneration Corp.      10-K  1604868   \n",
       "3   16371  Advanced Biomedical Technologies Inc.      10-K  1385799   \n",
       "4  230264                        Primerica, Inc.      10-K  1475922   \n",
       "\n",
       "         date                                         file  \\\n",
       "0  2017-03-15  edgar/data/1133311/0001133311-17-000010.txt   \n",
       "1  2017-02-27   edgar/data/712034/0000712034-17-000012.txt   \n",
       "2  2017-03-31  edgar/data/1604868/0001213900-17-003102.txt   \n",
       "3  2017-02-14  edgar/data/1385799/0001387131-17-000831.txt   \n",
       "4  2017-02-27  edgar/data/1475922/0001564590-17-002594.txt   \n",
       "\n",
       "                                              corpus  \n",
       "0  b'<SEC-DOCUMENT>0001133311-17-000010.txt : 201...  \n",
       "1  b'<SEC-DOCUMENT>0000712034-17-000012.txt : 201...  \n",
       "2  b'<SEC-DOCUMENT>0001213900-17-003102.txt : 201...  \n",
       "3  b'<SEC-DOCUMENT>0001387131-17-000831.txt : 201...  \n",
       "4  b'<SEC-DOCUMENT>0001564590-17-002594.txt : 201...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing the html tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files we have imported have html tags and it is very dificult to read, lets tale a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'15\\nDATE AS OF CHANGE:\\t\\t20170315\\n\\nFILER:\\n\\n\\tCOMPANY DATA:\\t\\n\\t\\tCOMPANY CONFORMED NAME:\\t\\t\\tTRAVELZOO INC\\n\\t\\tCENTRAL INDEX KEY:\\t\\t\\t0001133311\\n\\t\\tSTANDARD INDUSTRIAL CLASSIFICATION:\\tSERVICES-COMPUTER INTEGRATED SYSTEMS DESIGN [7373]\\n\\t\\tIRS NUMBER:\\t\\t\\t\\t364415727\\n\\t\\tSTATE OF INCORPORATION:\\t\\t\\tDE\\n\\t\\tFISCAL YEAR END:\\t\\t\\t1231\\n\\n\\tFILING VALUES:\\n\\t\\tFORM TYPE:\\t\\t10-K\\n\\t\\tSEC ACT:\\t\\t1934 Act\\n\\t\\tSEC FILE NUMBER:\\t000-50171\\n\\t\\tFILM N'\n"
     ]
    }
   ],
   "source": [
    "sample = data['corpus'][0]\n",
    "print(sample[300:700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_process( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string and the output is a single string\n",
    "    \n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review,\"html5lib\").get_text() \n",
    "    \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub('\\s+', ' ',review_text)    \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'orporation de fiscal year end filing values form type k sec act act sec file number film number business address street madison avenue street th floor city new york state ny zip business phone mail address street madison avenue street th floor city new york state ny zip k tzoo x k htm k document united statessecurities exchange commissionwashington c form k mark one xannual report pursuant section'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to make sure its working\n",
    "sample_clean = data['corpus'].head(1).apply(text_process)\n",
    "list(sample_clean)[0][300:700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% ... Documents processed: 100 time running: 105 minutes."
     ]
    }
   ],
   "source": [
    "# Get the number of documents based on the dataframe column size\n",
    "num_documents = data['corpus'].count()\n",
    "corpus = data['corpus']\n",
    "clean_corpus = []\n",
    "\n",
    "# Loop over each document; create an index i that goes from 0 to the \n",
    "#length of documents \n",
    "for i in range( 0, num_documents ):\n",
    "    # Call our function for each one, and add the result to the \n",
    "    #list of clena documents\n",
    "    clean_corpus.append( text_process( corpus[i] ) )\n",
    "    \n",
    "    # Printing out the the progress. \n",
    "    try:\n",
    "        count += 1\n",
    "        end = time.time()\n",
    "    except:\n",
    "        import sys \n",
    "        import time\n",
    "        start = time.time()\n",
    "        end = time.time()\n",
    "        count = 1\n",
    "        \n",
    "        #Need to change this variavel to match with the range of the for loop\n",
    "        num_documents = data['corpus'].count() \n",
    "        \n",
    "    sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * count/float(num_documents)) \\\n",
    "                     + \"% ... Documents processed: \" + str(count) \\\n",
    "                     + \" time running: \" + str(int((end-start)/60)) + \" minutes.\") \n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['corpus'] = clean_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets save the file already clened, so next time we can start from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_pickle('./data/10k-clean.pik')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the new data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_pickle('./data/10k-clean.pik')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Stemmed and Lemmatized Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% ... Documents processed: 100 time running: 33 minutes."
     ]
    }
   ],
   "source": [
    "#Creating a new collum for the corpus stemmed and lemmatized\n",
    "corpus_stemed = []\n",
    "corpus_lemmatized = []\n",
    "\n",
    "for corpus in data['corpus']:\n",
    "    corpus_stemed.append(\" \".join([stemmer.stem(word) \\\n",
    "                                   for word in corpus.split()]))\n",
    "    corpus_lemmatized.append(\" \".join([WordNetLemmatizer.lemmatize(word,word) \\\n",
    "                                   for word in corpus.split()]))\n",
    "    \n",
    "    # Printing out the the progress. \n",
    "    try:\n",
    "        count += 1\n",
    "        end = time.time()\n",
    "    except:\n",
    "        import sys \n",
    "        import time\n",
    "        start = time.time()\n",
    "        end = time.time()\n",
    "        count = 1\n",
    "        \n",
    "        #Need to change this variavel to match with the range of the for loop\n",
    "        num_documents = data['corpus'].count() \n",
    "        \n",
    "    sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * count/float(num_documents)) \\\n",
    "                     + \"% ... Documents processed: \" + str(count) \\\n",
    "                     + \" time running: \" + str(int((end-start)/60)) + \" minutes.\") \n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    \n",
    "data['stemmed'] = corpus_stemed\n",
    "data['lemmatized'] = corpus_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Word Count and Unique Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to calculate word count\n",
    "def word_count(data):\n",
    "    return len(data.split())\n",
    "\n",
    "#Function to calculate unique words\n",
    "def unique_count(data):\n",
    "    return len(set(data.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a new collum for the corpus word count and unique words\n",
    "data['word count'] = data['corpus'].apply(word_count)\n",
    "\n",
    "#Creating a new collumn for uniques words\n",
    "data['unique words'] = data['corpus'].apply(unique_count)\n",
    "data['unique stemmed'] = data['stemmed'].apply(unique_count)\n",
    "data['unique lemmatizes'] = data['lemmatized'].apply(unique_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Lets Count Negatives Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_neg = 'https://raw.githubusercontent.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107/08a269765a6b185d5f3dd522c876043ba9628715/data/opinion-lexicon-English/negative-words.txt'\n",
    "neg_words = pd.read_fwf(url_neg,encoding='latin-1',skiprows=34,header=None, \n",
    "                       names=['negatives'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_stemmed = []\n",
    "neg_lemmatized = []\n",
    "\n",
    "for word in neg_words['negatives']:\n",
    "    neg_stemmed.append(stemmer.stem(word))\n",
    "    neg_lemmatized.append(WordNetLemmatizer.lemmatize(word,word))\n",
    "\n",
    "neg_words['stemm'] = neg_stemmed\n",
    "neg_words['lemmatizes'] = neg_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negatives</th>\n",
       "      <th>stemm</th>\n",
       "      <th>lemmatizes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2-faced</td>\n",
       "      <td>2-face</td>\n",
       "      <td>2-faced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2-faces</td>\n",
       "      <td>2-face</td>\n",
       "      <td>2-faces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abnormal</td>\n",
       "      <td>abnorm</td>\n",
       "      <td>abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abolish</td>\n",
       "      <td>abolish</td>\n",
       "      <td>abolish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abominable</td>\n",
       "      <td>abomin</td>\n",
       "      <td>abominable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    negatives    stemm  lemmatizes\n",
       "0     2-faced   2-face     2-faced\n",
       "1     2-faces   2-face     2-faces\n",
       "2    abnormal   abnorm    abnormal\n",
       "3     abolish  abolish     abolish\n",
       "4  abominable   abomin  abominable"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_word = set(list(neg_words['negatives']))\n",
    "neg_stemmed = set(list(neg_words['stemm']))\n",
    "neg_lemmatizes = set(list(neg_words['lemmatizes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negative_count (data, negative=neg_word):\n",
    "   return len([word for word in data.split() if word in negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['Negative words'] = data['corpus'].apply(negative_count,neg_word)\n",
    "data['Negative Stemmed'] = data['stemmed'].apply(negative_count,negative=neg_stemmed)\n",
    "data['Negative lemmatized'] = data['lemmatized'].apply(negative_count,negative=neg_lemmatizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets save our database for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_pickle('./data/10k-v2.pik')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will export some columns to csv file, first we will select the columns, than export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name</th>\n",
       "      <th>word count</th>\n",
       "      <th>unique words</th>\n",
       "      <th>unique stemmed</th>\n",
       "      <th>unique lemmatizes</th>\n",
       "      <th>Negative words</th>\n",
       "      <th>Negative Stemmed</th>\n",
       "      <th>Negative lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAVELZOO INC</td>\n",
       "      <td>327222</td>\n",
       "      <td>12039</td>\n",
       "      <td>10608</td>\n",
       "      <td>11529</td>\n",
       "      <td>1733</td>\n",
       "      <td>4309</td>\n",
       "      <td>3179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACCO BRANDS Corp</td>\n",
       "      <td>841296</td>\n",
       "      <td>28456</td>\n",
       "      <td>25464</td>\n",
       "      <td>27566</td>\n",
       "      <td>5243</td>\n",
       "      <td>12004</td>\n",
       "      <td>10317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GrowGeneration Corp.</td>\n",
       "      <td>175052</td>\n",
       "      <td>11434</td>\n",
       "      <td>10226</td>\n",
       "      <td>11011</td>\n",
       "      <td>1178</td>\n",
       "      <td>3157</td>\n",
       "      <td>2550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Advanced Biomedical Technologies Inc.</td>\n",
       "      <td>82050</td>\n",
       "      <td>8008</td>\n",
       "      <td>6845</td>\n",
       "      <td>7601</td>\n",
       "      <td>889</td>\n",
       "      <td>2321</td>\n",
       "      <td>1635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Primerica, Inc.</td>\n",
       "      <td>132391</td>\n",
       "      <td>10091</td>\n",
       "      <td>7831</td>\n",
       "      <td>9352</td>\n",
       "      <td>3017</td>\n",
       "      <td>6600</td>\n",
       "      <td>4301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            company_name  word count  unique words  \\\n",
       "0                          TRAVELZOO INC      327222         12039   \n",
       "1                       ACCO BRANDS Corp      841296         28456   \n",
       "2                   GrowGeneration Corp.      175052         11434   \n",
       "3  Advanced Biomedical Technologies Inc.       82050          8008   \n",
       "4                        Primerica, Inc.      132391         10091   \n",
       "\n",
       "   unique stemmed  unique lemmatizes  Negative words  Negative Stemmed  \\\n",
       "0           10608              11529            1733              4309   \n",
       "1           25464              27566            5243             12004   \n",
       "2           10226              11011            1178              3157   \n",
       "3            6845               7601             889              2321   \n",
       "4            7831               9352            3017              6600   \n",
       "\n",
       "   Negative lemmatized  \n",
       "0                 3179  \n",
       "1                10317  \n",
       "2                 2550  \n",
       "3                 1635  \n",
       "4                 4301  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['company_name','word count','unique words','unique stemmed', \\\n",
    "      'unique lemmatizes', 'Negative words', 'Negative Stemmed', \\\n",
    "      'Negative lemmatized']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#exporting\n",
    "data[['company_name','word count','unique words','unique stemmed', \\\n",
    "      'unique lemmatizes', 'Negative words', 'Negative Stemmed', \\\n",
    "      'Negative lemmatized']].to_csv('./data/word count.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
